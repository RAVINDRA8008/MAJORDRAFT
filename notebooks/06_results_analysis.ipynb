{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e90ed898",
   "metadata": {},
   "source": [
    "# 06 â€” Results Analysis & Final Report\n",
    "\n",
    "Consolidate all results: confusion matrices, t-SNE embeddings,\n",
    "comparison tables, and export final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131cf376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "os.chdir('/content/amers')\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display, Markdown\n",
    "\n",
    "DRIVE_BASE = Path('/content/drive/MyDrive/AMERS')\n",
    "OUT = DRIVE_BASE / 'outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e03520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix\n",
    "cm_path = OUT / 'confusion_matrix.png'\n",
    "if cm_path.exists():\n",
    "    display(Image(str(cm_path), width=600))\n",
    "else:\n",
    "    print('Run evaluate.py first to generate confusion matrix.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3925e34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display t-SNE\n",
    "tsne_path = OUT / 'tsne_embeddings.png'\n",
    "if tsne_path.exists():\n",
    "    display(Image(str(tsne_path), width=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc79d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training curves\n",
    "for name in ['gan_loss.png', 'speech_loss.png', 'speech_acc.png', \n",
    "             'fusion_loss.png', 'fusion_acc.png', 'rl_aug_ratios.png',\n",
    "             'rl_analysis.png']:\n",
    "    p = OUT / name\n",
    "    if p.exists():\n",
    "        print(f'\\n--- {name} ---')\n",
    "        display(Image(str(p), width=700))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839f891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Markdown report\n",
    "report_path = OUT / 'report.md'\n",
    "if report_path.exists():\n",
    "    display(Markdown(report_path.read_text()))\n",
    "else:\n",
    "    print('No report found. Run evaluate.py first.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dd21fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison table\n",
    "import json\n",
    "\n",
    "json_path = OUT / 'results.json'\n",
    "if json_path.exists():\n",
    "    with open(json_path) as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    om = results.get('overall_metrics', {})\n",
    "    if om:\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"FINAL RESULTS\")\n",
    "        print(f\"{'='*40}\")\n",
    "        print(f\"Accuracy:       {om.get('accuracy', 'N/A'):.4f}\")\n",
    "        print(f\"F1 (macro):     {om.get('f1_macro', 'N/A'):.4f}\")\n",
    "        print(f\"F1 (weighted):  {om.get('f1_weighted', 'N/A'):.4f}\")\n",
    "        print(f\"Cohen's Kappa:  {om.get('kappa', 'N/A'):.4f}\")\n",
    "        print(f\"{'='*40}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
